# Web Scraper - Dockerized Multi-Stage Application

This project is a simple web scraping and hosting application that uses two different technologies in a multi-stage Docker build:

- A **Node.js (Puppeteer)** script to perform web scraping.
- A **Python (Flask)** app to host the scraped data as a JSON response.

The purpose is to demonstrate modular containerized architecture using Docker, where one stage handles scraping and the second serves the results via an API.

## Features

- Accepts a URL as an environment variable and performs scraping using Puppeteer and Chromium.
- Extracts metadata such as page title, meta description, and all hyperlinks from the page.
- Saves the output in a JSON format.
- A Flask application serves the scraped data at the root endpoint.
- Graceful error handling for invalid or unreachable URLs.
- Lightweight final image using multi-stage Docker builds.


## Project Structure

```
.
├── Dockerfile
├── README.md
├── scraper
│   ├── package.json
│   └── scrape.js
└── server
    ├── requirements.txt
    └── server.py
```


## How the Application Works

1. **Scraper Stage (Node.js)**
   - Based on the `node:18-slim` image.
   - Installs Puppeteer and required Chromium dependencies.
   - Executes `scrape.js`, which:
     - Reads the `SCRAPE_URL` environment variable.
     - Opens the given URL using headless Chromium.
     - Extracts the page title, meta description, and all links.
     - Saves the output into a `scraped_data.json` file.

2. **Flask Server Stage (Python)**
   - Based on the `python:3.10-slim` image.
   - Installs Flask from `requirements.txt`.
   - Copies the JSON file generated by the scraper.
   - Runs `server.py`, which:
     - Starts a Flask app on port 5000.
     - Serves the JSON data at the `/` endpoint.


## Prerequisites

- Docker must be installed and running on your machine.

## How to Build the Docker Image

From the root directory, run:

```
docker build -t web-scraper .
```


## How to Run the Container

To scrape the default URL (`https://exactspace.co/`) and host the data:

```
docker run -p 5000:5000 web-scraper
```

To scrape a custom URL:

```
docker run -p 5000:5000 -e SCRAPE_URL="https://www.example.com" web-scraper
```

Got it! Here's the updated **"Accessing the API"** section with the corrected note about the `sampleUrls` field showing a maximum of 10 URLs:


## Accessing the API

Once the container is running, open your browser or API client and visit:

```
http://localhost:5000
```

This endpoint returns the scraped data in JSON format after a successful operation, including:

- `title`: Title of the web page  
- `metaDescription`: Meta description content (if available)  
- `url`: The URL that was scraped  
- `timestamp`: ISO timestamp of when the data was scraped  
- `totalUrlsFound`: Number of hyperlinks found on the page  
- `sampleUrls`: A sample list of up to **10** extracted URLs from the page  

To check if the server is active and running, you can use the following health-check endpoint:

```
http://localhost:5000/health
```

This will return a simple JSON response:

```json
{"status": "ok"}
```
This confirms that the server is up and ready to handle requests.


## Error Handling

If an invalid or unreachable URL is provided, the script will log a warning:

```
Error during scraping: Invalid or unreachable URL. Please check the URL and try again.
```

In this case, the Flask server will still start, but the output JSON may not contain valid data. Users are advised to provide a valid, reachable URL to get accurate results.


